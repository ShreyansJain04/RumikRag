# Default configuration for RAG QA system

# Paths
data_dir: "data"
raw_data_dir: "data/raw"
processed_data_dir: "data/processed"
index_dir: "data/indexes"
checkpoint_dir: "checkpoints"
results_dir: "results"

# Models
t5_model: "google/flan-t5-large"
causal_model: "mistralai/Mistral-7B-Instruct-v0.2"
embedding_model: "BAAI/bge-small-en-v1.5"
reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"

# Training
batch_size: 8
learning_rate: 2e-4
epochs: 3
max_source_length: 1024
max_target_length: 32
max_prompt_length: 1536
max_answer_length: 64
gradient_accumulation_steps: 1
warmup_steps: 100
save_steps: 500
eval_steps: 500
logging_steps: 100

# Retrieval
chunk_size: 256
chunk_stride: 50
top_k: 5
rerank_top_k: 10
rerank_candidates: 50
hybrid_alpha: 0.5  # Weight for dense retrieval in hybrid (1-alpha for BM25)

# LoRA
lora_r: 16
lora_alpha: 32
lora_dropout: 0.1
lora_target_modules: ["q", "v"]  # For T5
causal_lora_target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]  # For causal models

# Generation
num_beams: 4
temperature: 0.7
do_sample: false

# Evaluation
eval_batch_size: 16
metrics: ["exact_match", "f1", "recall_at_k"]

# System
seed: 42
device: "cuda"
fp16: true
bf16: false
gradient_checkpointing: true

